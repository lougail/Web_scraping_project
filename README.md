# Web Scraping Project â€” SystÃ¨me de veille concurrentielle

## ğŸ¯ Description

Ce projet est un systÃ¨me de veille concurrentielle centrÃ© sur la collecte et lâ€™analyse de donnÃ©es issues dâ€™un site de livres ([books.toscrape.com](https://books.toscrape.com)).  
Il combine un composant de **scraping** (via Scrapy) et une **API REST** (via FastAPI) pour exposer les donnÃ©es collectÃ©es et proposer des statistiques.

### FonctionnalitÃ©s principales

- Scraping automatique de plus de **1000 livres** Ã  partir de *books.toscrape.com*  
- Nettoyage et validation des donnÃ©es via pipelines Scrapy  
- Stockage dans une base de donnÃ©es **SQLite** via **SQLAlchemy**  
- API REST (FastAPI) structurÃ©e selon les principes de la Clean Architecture  
- Endpoints de consultation des livres + endpoints de statistiques (moyennes, top catÃ©gories, etc.)  

---

## ğŸ“ Architecture du projet

```
books-intelligence/
â”œâ”€â”€ books_scraper/        # Projet Scrapy
â”‚   â”œâ”€â”€ spiders/           # Les spiders pour le scraping
â”‚   â”œâ”€â”€ pipelines.py       # Nettoyage, validation, insertion en DB
â”‚   â””â”€â”€ database/          # ModÃ¨les SQLAlchemy & gestion DB
â””â”€â”€ app/                   # API FastAPI (Clean Architecture)
    â”œâ”€â”€ routers/           # DÃ©finition des routes HTTP / endpoints
    â”œâ”€â”€ services/          # Logique mÃ©tier (cas dâ€™usage)
    â”œâ”€â”€ repositories/      # AccÃ¨s aux donnÃ©es / abstraction DB
    â”œâ”€â”€ schemas/           # ModÃ¨les Pydantic (validation)
    â””â”€â”€ database/          # Configuration de la connexion DB
```

---

## ğŸ”§ PrÃ©requis

- Python 3.11 ou supÃ©rieur  
- Git  
- (Optionnel mais recommandÃ©) un environnement virtuel  

---

## ğŸš€ Installation & utilisation

### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/lougail/Web_scraping_project.git
cd Web_scraping_project
```

### 2. CrÃ©er un environnement virtuel et installer les dÃ©pendances
```bash
python -m venv venv
# Sous Linux/Mac
source venv/bin/activate
# Sous Windows
venv\Scripts\activate

pip install -e .
```

### 3. Lancer le scraping
```bash
cd books_scraper
scrapy crawl books
```
â¡ï¸ Les donnÃ©es collectÃ©es seront stockÃ©es dans `books_scraper/books.db`.

### 4. Lancer lâ€™API FastAPI
Depuis la racine du projet :
```bash
uvicorn app.main:app --reload
```
- API disponible : [http://127.0.0.1:8000](http://127.0.0.1:8000)  
- Documentation interactive : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  

---

## ğŸ§° Endpoints de lâ€™API

### Livres
- `GET /books` : liste paginÃ©e de livres (`page`, `per_page`, `category`)  
- `GET /books/{id}` : dÃ©tails dâ€™un livre

### Statistiques
- `GET /stats/general` : statistiques globales (nb de livres, prix moyen, etc.)  
- `GET /stats/top-categories` : top catÃ©gories par nombre de livres  
- `GET /stats/price-by-category` : prix moyen par catÃ©gorie  

#### Exemples avec `curl`
```bash
# Obtenir les 20 premiers livres
curl http://localhost:8000/books

# Filtrer par catÃ©gorie â€œFictionâ€
curl http://localhost:8000/books?category=Fiction

# Obtenir le prix moyen
curl http://localhost:8000/stats/general

# Top 10 catÃ©gories
curl http://localhost:8000/stats/top-categories?limit=10
```

---

## ğŸ“Š Structure des donnÃ©es collectÃ©es

- Titre  
- Prix  
- Note (1 Ã  5 Ã©toiles)  
- CatÃ©gorie  
- Description  
- Stock disponible  
- UPC (identifiant unique)  
- Nombre de reviews  
- URL de lâ€™image (couverture)  

---

## ğŸ§© Stack technique

- **Scraping** : Scrapy  
- **Base de donnÃ©es** : SQLite + SQLAlchemy  
- **API** : FastAPI  
- **Validation** : Pydantic  
- **Serveur** : Uvicorn  

---

## ğŸ›ï¸ Principes dâ€™architecture

- **Clean Architecture** : sÃ©paration claire des couches (routers â†’ services â†’ repositories â†’ modÃ¨les)  
- **Injection de dÃ©pendances** (via FastAPI)  
- **Repository Pattern** pour abstraire lâ€™accÃ¨s aux donnÃ©es  
- **Validation stricte** via Pydantic  

---

## ğŸ§¾ Licence

Ce projet est distribuÃ© sous licence **MIT**.  
Auteur : Lougail  

---

## ğŸ“Œ AmÃ©liorations possibles

- Ajout de tests unitaires et dâ€™intÃ©gration  
- Scraping multi-sources (plusieurs sites de livres)  
- Planification automatisÃ©e (cron, Airflow, etc.)  
- Authentification et gestion des rÃ´les sur lâ€™API  
- Migration vers une base plus robuste (PostgreSQL)  
- DÃ©ploiement en production (Docker, cloud, CI/CD)  
