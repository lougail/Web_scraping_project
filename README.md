# Web Scraping Project â€” SystÃ¨me de veille concurrentielle

![Python](https://img.shields.io/badge/Python-3.11%2B-blue)
![Build](https://img.shields.io/badge/build-passing-brightgreen)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## ğŸ¯ Description

Ce projet est un systÃ¨me de veille concurrentielle centrÃ© sur la collecte et lâ€™analyse de donnÃ©es issues dâ€™un site de livres ([books.toscrape.com](https://books.toscrape.com)).  
Il combine un composant de **scraping** (via Scrapy) et une **API REST** (via FastAPI) pour exposer les donnÃ©es collectÃ©es et proposer des statistiques avancÃ©es.

---

## ğŸš€ FonctionnalitÃ©s principales

- Scraping automatique de plus de **1000 livres** Ã  partir de *books.toscrape.com*
- Nettoyage et validation des donnÃ©es via pipelines Scrapy
- Stockage dans une base de donnÃ©es **SQLite** via **SQLAlchemy**
- **SystÃ¨me d'historique** : tracking automatique des changements de prix, stock, rating et reviews
- API REST (FastAPI) structurÃ©e selon les principes de la Clean Architecture
- **Recherche avancÃ©e** : multi-critÃ¨res avec filtres (prix, rating, catÃ©gorie, texte)
- **Endpoints d'historique** : Ã©volution des prix, alertes stock, changements rÃ©cents
- Endpoints de statistiques avancÃ©es (distribution des notes, tranches de prix, etc.)
- **Rate limiting** et middleware (CORS, compression GZip)
- Pagination complÃ¨te avec mÃ©tadonnÃ©es (total, pages, etc.)
- Tri et ordonnancement dynamiques
- Documentation interactive complÃ¨te (Swagger UI)

---

## ğŸ—ï¸ Architecture du projet

```
books-intelligence/
â”œâ”€â”€ books_scraper/         # Projet Scrapy : scraping & nettoyage
â”‚   â”œâ”€â”€ books_scraper/
â”‚   â”‚   â”œâ”€â”€ spiders/       # Les spiders pour le scraping
â”‚   â”‚   â”œâ”€â”€ pipelines.py   # Nettoyage, validation, insertion en DB + historique
â”‚   â”‚   â”œâ”€â”€ settings.py    # ParamÃ©trage Scrapy (throttling, cache, etc.)
â”‚   â”‚   â”œâ”€â”€ constants.py   # Constantes (RATING_MAP, URLs, etc.)
â”‚   â”‚   â””â”€â”€ database/      # ModÃ¨les SQLAlchemy & gestion DB
â”‚   â””â”€â”€ books.db           # Base de donnÃ©es SQLite
â”œâ”€â”€ app/                   # API FastAPI (Clean Architecture)
â”‚   â”œâ”€â”€ routers/           # Routes HTTP / endpoints
â”‚   â”‚   â”œâ”€â”€ books.py       # Endpoints livres (liste, dÃ©tail, search, categories, random)
â”‚   â”‚   â”œâ”€â”€ stats.py       # Endpoints statistiques
â”‚   â”‚   â””â”€â”€ history.py     # Endpoints historique (prix, stock, changements)
â”‚   â”œâ”€â”€ services/          # Logique mÃ©tier (cas d'usage)
â”‚   â”œâ”€â”€ repositories/      # AccÃ¨s aux donnÃ©es / abstraction DB
â”‚   â”œâ”€â”€ schemas/           # ModÃ¨les Pydantic (validation)
â”‚   â”œâ”€â”€ database/          # Configuration connexion DB & modÃ¨les
â”‚   â”œâ”€â”€ config.py          # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ error_handlers.py  # Gestion centralisÃ©e des erreurs
â”‚   â””â”€â”€ main.py            # Point d'entrÃ©e FastAPI
â”œâ”€â”€ tests/                 # Tests unitaires et d'intÃ©gration (26 tests)
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ pyproject.toml         # Configuration du projet
â”œâ”€â”€ Dockerfile             # Conteneurisation
â”œâ”€â”€ Makefile               # Commandes automatisÃ©es
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©requis

- Python 3.11 ou supÃ©rieur  
- Git  
- (Optionnel mais recommandÃ©) un environnement virtuel  
- SQLite (installÃ© par dÃ©faut sur la plupart des systÃ¨mes)
- (Bonus) Docker pour la conteneurisation

---

## ğŸš¦ Installation & utilisation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/lougail/Web_scraping_project.git
cd Web_scraping_project
```

### 2. CrÃ©er un environnement virtuel et installer les dÃ©pendances

```bash
python -m venv venv
# Sous Linux/Mac
source venv/bin/activate
# Sous Windows
venv\Scripts\activate

pip install -e .
```

### 3. Lancer le scraping

```bash
cd books_scraper
scrapy crawl books
```
â¡ï¸ Les donnÃ©es collectÃ©es seront stockÃ©es dans `books_scraper/books.db`.

### 4. Lancer l'API FastAPI

Depuis la racine du projet :

```bash
uvicorn app.main:app --reload
```
- API disponible : [http://127.0.0.1:8000](http://127.0.0.1:8000)
- Documentation interactive : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### 5. (Optionnel) Lancer les tests

```bash
pytest tests/
```

### 6. (Optionnel) Utiliser Docker

```bash
docker build -t books-intelligence .
docker run -p 8000:8000 books-intelligence
```

### 7. (Optionnel) Utiliser le Makefile

Lance les commandes d'un coup avec :

```bash
make scrape     # Lancer le scraping
make api        # Lancer l'API
make test       # Lancer les tests
make all        # Tout faire
```

---

## ğŸ“š Endpoints de l'API

### ğŸ“– Livres

- `GET /books` : liste paginÃ©e de livres avec tri et filtres
  - ParamÃ¨tres : `page`, `per_page`, `category`, `sort_by`, `order`
- `GET /books/{id}` : dÃ©tails d'un livre spÃ©cifique
- `GET /books/count` : nombre total de livres
- `GET /books/search` : recherche avancÃ©e multi-critÃ¨res
  - ParamÃ¨tres : `q` (texte), `category`, `min_price`, `max_price`, `min_rating`, `max_rating`, `sort_by`, `order`
- `GET /books/categories` : liste de toutes les catÃ©gories disponibles
- `GET /books/random` : obtenir des livres alÃ©atoires
  - ParamÃ¨tres : `limit` (dÃ©faut: 10, max: 50)

### ğŸ“Š Statistiques

- `GET /stats/general` : statistiques globales (nb livres, prix moyen, etc.)
- `GET /stats/top-categories` : top catÃ©gories par nombre de livres
  - ParamÃ¨tres : `limit` (dÃ©faut: 10)
- `GET /stats/price-by-category` : prix moyen par catÃ©gorie
- `GET /stats/rating-distribution` : distribution des notes (1-5 Ã©toiles)
- `GET /stats/price-ranges` : rÃ©partition des livres par tranches de prix

### ğŸ“ˆ Historique

- `GET /history/books/{book_id}` : historique complet d'un livre
  - ParamÃ¨tres : `days` (limiter aux N derniers jours), `limit`
- `GET /history/books/{book_id}/price` : Ã©volution du prix dans le temps
  - ParamÃ¨tres : `days`
- `GET /history/price-changes` : livres avec changements de prix rÃ©cents
  - ParamÃ¨tres : `days` (dÃ©faut: 7), `limit` (dÃ©faut: 50)
- `GET /history/stock-alerts` : livres en rupture ou stock faible
  - ParamÃ¨tres : `threshold` (dÃ©faut: 10)

### ğŸ¥ SantÃ©

- `GET /` : informations sur l'API et liste des endpoints
- `GET /health` : health check (status + connexion DB)

#### Exemples avec `curl`

```bash
# Liste paginÃ©e avec tri par prix croissant
curl "http://localhost:8000/books?page=1&per_page=10&sort_by=price&order=asc"

# Recherche avancÃ©e : livres "Python" entre 10â‚¬ et 50â‚¬, note â‰¥ 4
curl "http://localhost:8000/books/search?q=python&min_price=10&max_price=50&min_rating=4"

# Toutes les catÃ©gories disponibles
curl http://localhost:8000/books/categories

# 5 livres alÃ©atoires
curl "http://localhost:8000/books/random?limit=5"

# Statistiques gÃ©nÃ©rales
curl http://localhost:8000/stats/general

# Distribution des notes
curl http://localhost:8000/stats/rating-distribution

# Historique complet d'un livre (30 derniers jours)
curl "http://localhost:8000/history/books/1?days=30"

# Ã‰volution du prix d'un livre
curl "http://localhost:8000/history/books/1/price?days=90"

# Changements de prix des 7 derniers jours
curl "http://localhost:8000/history/price-changes?days=7&limit=20"

# Alertes stock (livres avec stock â‰¤ 5)
curl "http://localhost:8000/history/stock-alerts?threshold=5"
```

---

## ğŸ—ƒï¸ Structure de la base de donnÃ©es

### Table `books`
- `id` : Identifiant unique (auto-incrÃ©mentÃ©)
- `upc` : Universal Product Code (identifiant unique du livre)
- `title` : Titre du livre
- `price` : Prix (float)
- `rating` : Note de 1 Ã  5 Ã©toiles (int)
- `category` : CatÃ©gorie du livre
- `description` : Description dÃ©taillÃ©e
- `stock` : Stock disponible (int)
- `number_of_reviews` : Nombre d'avis
- `image_url` : URL de l'image de couverture
- `scraped_at` : Date du dernier scraping
- `last_updated` : Date de derniÃ¨re modification

### Table `book_history`
- `id` : Identifiant unique
- `book_id` : RÃ©fÃ©rence au livre (foreign key)
- `upc` : UPC du livre
- `price` : Prix Ã  ce moment
- `stock` : Stock Ã  ce moment
- `rating` : Note Ã  ce moment
- `number_of_reviews` : Nombre d'avis Ã  ce moment
- `scraped_at` : Date de l'enregistrement

**Index optimisÃ©s** : book_id, upc, scraped_at, composites (book_id + scraped_at, upc + scraped_at)  

---

## ğŸ§‘â€ğŸ’» Stack technique

- **Scraping** : Scrapy 2.13
- **Base de donnÃ©es** : SQLite + SQLAlchemy 2.0
- **API** : FastAPI 0.115
- **Validation** : Pydantic 2.10 + Pydantic Settings
- **Serveur** : Uvicorn 0.34
- **Rate Limiting** : SlowAPI (in-memory)
- **Middleware** : CORS, GZip compression
- **Tests** : Pytest 7.4 + httpx + pytest-asyncio (26 tests)
- **Linting/formatting** : Black, Ruff
- **Conteneurisation** : Docker

---

## ğŸ›ï¸ Principes d'architecture

- **Clean Architecture** : sÃ©paration claire des couches (routers â†’ services â†’ repositories â†’ modÃ¨les)
- **Injection de dÃ©pendances** (via FastAPI)
- **Repository Pattern** pour abstraire l'accÃ¨s aux donnÃ©es
- **Validation stricte** via Pydantic avec schÃ©mas typÃ©s
- **Logging** et gestion d'erreurs centralisÃ©e
- **Pagination avancÃ©e** avec mÃ©tadonnÃ©es (total, pages, etc.)
- **Rate limiting** pour protection anti-abus
- **Historique automatique** : tracking des changements en temps rÃ©el via pipeline
- **Index DB optimisÃ©s** : composite indexes pour queries performantes
- **Code modulaire** : chaque couche a sa responsabilitÃ© unique

---

## ğŸ¥‡ Bonnes pratiques et automatisation

- **Formatage & lint** :  
  - Formater le code : `black .`
  - Linter le code : `ruff .`
- **Tests** :  
  - Lancer les tests unitaires : `pytest tests/`
- **Automatisation** (si Makefile prÃ©sent) :  
  - Scraping + API + tests : `make all`

---

## ğŸ§‘â€ğŸ”¬ AmÃ©liorations possibles

- ğŸ”„ Scraping multi-sources (plusieurs sites de livres)
- â° Planification automatisÃ©e (cron, Airflow, Celery pour scraping pÃ©riodique)
- ğŸ” Authentification JWT et gestion des rÃ´les (admin, user)
- ğŸ—„ï¸ Migration vers PostgreSQL pour production
- â˜ï¸ DÃ©ploiement cloud (AWS/Azure/GCP) avec CI/CD (GitHub Actions)
- ğŸ“Š Dashboard frontend (React/Vue) pour visualiser les donnÃ©es
- ğŸ” Search engine (Elasticsearch) pour recherche full-text avancÃ©e
- ğŸ“§ SystÃ¨me de notifications (email/webhook) pour alertes prix/stock
- ğŸ“ˆ Monitoring et observabilitÃ© (Prometheus/Grafana, Sentry)
- ğŸš€ Caching distribuÃ© (Redis) pour amÃ©liorer les performances
- ğŸ“ Webhooks pour Ã©vÃ©nements (nouveau livre, changement prix, etc.)

---

## ğŸ“ Licence

Ce projet est distribuÃ© sous licence **MIT**.  
Auteur : Lougail

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Proposez une issue ou une pull request pour discuter de vos idÃ©es ou corrections.  
Voir `CONTRIBUTING.md` (Ã  crÃ©er pour les guidelines de contribution).

---