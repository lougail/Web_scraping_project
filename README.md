# Web Scraping Project â€” SystÃ¨me de veille concurrentielle

![Python](https://img.shields.io/badge/Python-3.11%2B-blue)
![Build](https://img.shields.io/badge/build-passing-brightgreen)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## ğŸ¯ Description

Ce projet est un systÃ¨me de veille concurrentielle centrÃ© sur la collecte et lâ€™analyse de donnÃ©es issues dâ€™un site de livres ([books.toscrape.com](https://books.toscrape.com)).  
Il combine un composant de **scraping** (via Scrapy) et une **API REST** (via FastAPI) pour exposer les donnÃ©es collectÃ©es et proposer des statistiques avancÃ©es.

---

## ğŸš€ FonctionnalitÃ©s principales

- Scraping automatique de plus de **1000 livres** Ã  partir de *books.toscrape.com*  
- Nettoyage et validation des donnÃ©es via pipelines Scrapy  
- Stockage dans une base de donnÃ©es **SQLite** via **SQLAlchemy**  
- API REST (FastAPI) structurÃ©e selon les principes de la Clean Architecture  
- Endpoints de consultation des livres + endpoints de statistiques (moyennes, top catÃ©gories, etc.)  
- Prise en charge de la pagination et des filtres
- Exemples de requÃªtes analytiques intÃ©grÃ©es

---

## ğŸ—ï¸ Architecture du projet

```
Web_scraping_project/
â”œâ”€â”€ books_scraper/         # Projet Scrapy : scraping & nettoyage
â”‚   â”œâ”€â”€ spiders/           # Les spiders pour le scraping
â”‚   â”œâ”€â”€ pipelines.py       # Nettoyage, validation, insertion en DB
â”‚   â”œâ”€â”€ items.py           # DÃ©finition des champs extraits
â”‚   â”œâ”€â”€ settings.py        # ParamÃ©trage Scrapy (throttling, user-agent, etc.)
â”‚   â””â”€â”€ database/          # ModÃ¨les SQLAlchemy & gestion DB
â”œâ”€â”€ app/                   # API FastAPI (Clean Architecture)
â”‚   â”œâ”€â”€ routers/           # DÃ©finition des routes HTTP / endpoints
â”‚   â”œâ”€â”€ services/          # Logique mÃ©tier (cas dâ€™usage)
â”‚   â”œâ”€â”€ repositories/      # AccÃ¨s aux donnÃ©es / abstraction DB
â”‚   â”œâ”€â”€ schemas/           # ModÃ¨les Pydantic (validation)
â”‚   â”œâ”€â”€ database/          # Configuration de la connexion DB
â”‚   â””â”€â”€ error_handlers.py  # Gestion centralisÃ©e des erreurs (Ã  ajouter)
â”œâ”€â”€ tests/                 # (Bonus) Tests unitaires et dâ€™intÃ©gration
â”œâ”€â”€ pyproject.toml         # DÃ©pendances et configuration du projet
â”œâ”€â”€ Dockerfile             # (Bonus) Conteneurisation
â”œâ”€â”€ Makefile               # (Bonus) Commandes automatisÃ©es
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©requis

- Python 3.11 ou supÃ©rieur  
- Git  
- (Optionnel mais recommandÃ©) un environnement virtuel  
- SQLite (installÃ© par dÃ©faut sur la plupart des systÃ¨mes)
- (Bonus) Docker pour la conteneurisation

---

## ğŸš¦ Installation & utilisation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/lougail/Web_scraping_project.git
cd Web_scraping_project
```

### 2. CrÃ©er un environnement virtuel et installer les dÃ©pendances

```bash
python -m venv venv
# Sous Linux/Mac
source venv/bin/activate
# Sous Windows
venv\Scripts\activate

pip install -e .
```

### 3. Lancer le scraping

```bash
cd books_scraper
scrapy crawl books
```
â¡ï¸ Les donnÃ©es collectÃ©es seront stockÃ©es dans `books_scraper/books.db`.

### 4. CrÃ©er / initialiser la base de donnÃ©es

```bash
python test_db.py
```

### 5. Lancer lâ€™API FastAPI

Depuis la racine du projet :

```bash
uvicorn app.main:app --reload
```
- API disponible : [http://127.0.0.1:8000](http://127.0.0.1:8000)  
- Documentation interactive : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  

### 6. (Optionnel) Lancer les tests

```bash
pytest tests/
```

### 7. (Optionnel) Utiliser Docker

```bash
docker build -t books-intelligence .
docker run -p 8000:8000 books-intelligence
```

### 8. (Optionnel) Utiliser le Makefile

Lance les commandes d'un coup avec :

```bash
make scrape     # Lancer le scraping
make api        # Lancer l'API
make test       # Lancer les tests
make all        # Tout faire
```

---

## ğŸ“š Endpoints de lâ€™API

### Livres

- `GET /books` : liste paginÃ©e de livres (`page`, `per_page`, `category`)  
- `GET /books/{id}` : dÃ©tails dâ€™un livre

### Statistiques

- `GET /stats/general` : statistiques globales (nb de livres, prix moyen, etc.)  
- `GET /stats/top-categories` : top catÃ©gories par nombre de livres  
- `GET /stats/price-by-category` : prix moyen par catÃ©gorie  

#### Exemples avec `curl`

```bash
# Obtenir les 20 premiers livres
curl http://localhost:8000/books

# Filtrer par catÃ©gorie â€œFictionâ€
curl http://localhost:8000/books?category=Fiction

# Obtenir le prix moyen
curl http://localhost:8000/stats/general

# Top 10 catÃ©gories
curl http://localhost:8000/stats/top-categories?limit=10
```

---

## ğŸ—ƒï¸ Structure des donnÃ©es collectÃ©es

- Titre  
- Prix  
- Note (1 Ã  5 Ã©toiles)  
- CatÃ©gorie  
- Description  
- Stock disponible  
- UPC (identifiant unique)  
- Nombre de reviews  
- URL de lâ€™image (couverture)  

---

## ğŸ§‘â€ğŸ’» Stack technique

- **Scraping** : Scrapy  
- **Base de donnÃ©es** : SQLite + SQLAlchemy  
- **API** : FastAPI  
- **Validation** : Pydantic  
- **Serveur** : Uvicorn  
- **Tests** : Pytest, httpx  
- **Linting/formatting** : Black, Ruff  
- **Conteneurisation** : Docker (optionnel)

---

## ğŸ›ï¸ Principes dâ€™architecture

- **Clean Architecture** : sÃ©paration claire des couches (routers â†’ services â†’ repositories â†’ modÃ¨les)  
- **Injection de dÃ©pendances** (via FastAPI)  
- **Repository Pattern** pour abstraire lâ€™accÃ¨s aux donnÃ©es  
- **Validation stricte** via Pydantic  
- **Logging** et gestion dâ€™erreurs robustes  
- **Pagination & filtres** sur les endpoints

---

## ğŸ¥‡ Bonnes pratiques et automatisation

- **Formatage & lint** :  
  - Formater le code : `black .`
  - Linter le code : `ruff .`
- **Tests** :  
  - Lancer les tests unitaires : `pytest tests/`
- **Automatisation** (si Makefile prÃ©sent) :  
  - Scraping + API + tests : `make all`

---

## ğŸ§‘â€ğŸ”¬ AmÃ©liorations possibles

- Ajout de tests unitaires et dâ€™intÃ©gration  
- Scraping multi-sources (plusieurs sites de livres)  
- Planification automatisÃ©e (cron, Airflow, etc.)  
- Authentification et gestion des rÃ´les sur lâ€™API  
- Migration vers une base plus robuste (PostgreSQL, Azure, etc.)  
- DÃ©ploiement en production (Docker, cloud, CI/CD)  
- Ajout de endpoints analytiques avancÃ©s (distribution de prix, histogramme, etc.)  
- Monitoring (Prometheus/Grafana)

---

## ğŸ“ Licence

Ce projet est distribuÃ© sous licence **MIT**.  
Auteur : Lougail

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Proposez une issue ou une pull request pour discuter de vos idÃ©es ou corrections.  
Voir `CONTRIBUTING.md` (Ã  crÃ©er pour les guidelines de contribution).

---